# SOC-25-Building-LLM-from-Scratch
In the project build LLM from Scratch, we first started with python, beginner friendly and language useful in various domains.
It is very effective in handling data and hence in used in LLMs due to its built in libraries mainly numpy, pytorch, Tensorflow, etc.
Started with learning basic syntax of python, elements in it like list, tuple, sets, etc. and then also file handling and classes in python.
Then libraries mainly numpy and pytorch. In NumPy, learn about arrays, slicing arrays, mathematical operations on them, and random number generation, file operations.
In Pytorch, learned about tensors, autograd, training loop, nn.module, dataset and dataloaders.

After this, we move on to learning about transformers, which has its own encoder and decoder. This Encoder and Decoder with the help of input and previous output provides us with outcome probabilities whicha re further used for the specific task the LLM need to perform. It has various process like multi-head attention, Normalization, Feed forward, taking palce to get the outcome.
The first stage of LLM architecture is Data preparation and sampling. Which starts with tokenizing the input text provided to the model.
Tokenizaation- word based, character based and sub-word based. Among these the subword based tokenizer is the best and is used in GPT model as BYTE PAIR ENCODING. In tokenization, text is split in this tokens and then assigned a unique Token ID for the specific token and this can be done using the tiktoken library of python. Before the next step of creating embeddings of these tokens the text is split in input- target pairs with the help of Dataset and DataLoader.
There are also special context tokens as <|unk|>: to handle new words in the target pair and <|endoftext|>: to join different unrelated text samples so that model can understand to different dataset are feed as a single text data.

Now the token IDs differs the words but the semantically similar words should have similar representation before feeding to the model, this is done by embedding the tokens to their respective vectors, which are created by using neural networks. Used the nn.embedding function in the nn.module to create token embeddings. 
There is also the need of positional embedding so that the model could relate where the word actually occurs in the text (same word at two different places must embedded  differently), which are calculated by creating arrange tensor and getting embeddings of it. Then this positional embeddings are added to the embeddings to get the final token embeddings.

For the relation between words in the text, the model should get the context between the words which will train our model effectively. This is done by attention mechanism. Before transformers, RNNs(recurrent neural networks) were the most popualar encoder-decoder architecture but the decoder in them can not access the output other than the previous output (decoder only used the final hidden state), leading to loss of context mainly in complex sentences. The problem is solved by the attention mechanism and now the decoder can access all input tokens selectively.Hence, some inputs are more important and their importance is determined by the attention weights. 

The Simplified Self Attention mechanism, allows each position in the input sequence to attend other positions while computing represention of it. In this the embeddings of the words(tokens) are multiplied by attention weights to get the context vector embeddings. The attention weights are gained using the attention scores, which are obtained through the dot product between the vectors and then applying Softmax function to normalize them and these scores after normalization are attention weights.
Then the modified Self attention with trainable weights, in which we get 3 trainable random weight matrices Wq, Wk, and Wv generated using nn.Parameter or better with nn.Linear with qkv_bias. After multiplying this with the input vector embedding matrix we get the Query, Key and Value matrix. the attention scores are obtained by multiplying Query with transpose of Keys, which are then divide by the underoot iof last dimension of key to decrease the variance which was high as random numbers are multiplied and then applying SoftMax to get the attention weights, then getting the context vector by multiplying attention weights with the values.
The Casual attention mechanism, just mask the elements above the diagonal in the attention scores and then normalize to get attention weights, further masking by using nn.Dropout and then normalizing and then multiplying with values to get the context vectors. 
Last modified is the multi-headf attention in which the attention mechanism is dividied into differnt heads, each having its own weights and then computing there outputs and concatenate them to get the ultimate context vector matrix. This is computationally intensive but it makes LLms powerful to recognize complex patterns.

Then using this we build a GPT initial model, in which we build the model GPT2 by performing the following steps:
Defining model hyperparameters and storing them in a python dictionary, using torch.nn.module  create a DummyGPTModel Class, then adding embedding layers (token and position), applying dropouts, creating transformer blocks with self-attention and feed forward, applying Layer Normalization to them, adding a outputr layer and implemented a forward pass which takes through all the above process to get the output.
